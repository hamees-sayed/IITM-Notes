# Estimation of parameters using ML  

**Introduction:**
- MLE is a method used to estimate unknown parameters from data of a statistical model.
- It's widely used in probability theory and statistics.
- Involves finding parameter values that maximize the likelihood function.
- The likelihood of the true parameters begin a certain value (given the data) is the same as the probability of observing the data (given some true parameter values).

**Parameter Estimation Framework:**
- Family of distributions denoted as $ \mathcal{P} $ parameterized by $ \theta $.
- Data $ x_1, x_2, \ldots, x_n $ drawn independently from $ \mathcal{P}_{\theta} $.

**Likelihood Function:**
- Likelihood $ L(\theta) $ measures how probable the observed data is for a given $ \theta $.
- $ L(\theta) $ is the product (or sum in log space) of probabilities of individual data points.

**Maximum Likelihood Estimation (MLE):**
- MLE seeks $ \hat{\theta} $ that maximizes $ L(\theta) $ or $ \log L(\theta) $.
- Often, it simplifies calculations to work with log-likelihood.  

The likelihood function, denoted by $ L(\theta) $, is the joint distribution function of the sample values $ x_1, x_2, \ldots, x_n $ when considered as a function of the parameter $ \theta $. It is often denoted as $ L(\theta) = f(x_1, \theta) \cdot f(x_2, \theta) \cdot \ldots \cdot f(x_n, \theta) $ when dealing with the density function $ f(x, \theta) $ associated with the population.

In mathematical terms, the likelihood function $ L(\theta) $ is expressed as:

$ L(\theta) = f(x_1, \theta) \cdot f(x_2, \theta) \cdot \ldots \cdot f(x_n, \theta) $  
$\text{OR}$  
$ L(\theta) = \prod_{i=1}^n f(x_i, \theta)$

This function represents the joint probability of observing the given sample values $ x_1, x_2, \ldots, x_n $ given the parameter $ \theta $. The likelihood function plays a crucial role in Maximum Likelihood Estimation (MLE), where the goal is to find the parameter value that maximizes the likelihood function, i.e., the value of $ \theta $ that makes the observed sample most probable.

**Examples:**

1. **Bernoulli Distribution (Coin Toss):**
   - Family: $ p_{\theta} $ is Bernoulli with bias $ \theta $.
   - MLE for $ \theta $: $ \hat{\theta}_{\text{MLE}} = \frac{\text{Number of Heads}}{\text{Total Tosses}} $.

2. **Uniform Distribution (Data Range):**
   - Family: $ p_{\theta} $ is Uniform in $[a, b]$.
   - MLE for $ a $ and $ b $: $ \hat{a}_{\text{MLE}} = \min(\text{Data}) $, $ \hat{b}_{\text{MLE}} = \max(\text{Data}) $.

3. **Normal Distribution (Sample Mean and Variance):**
   - Family: $ p_{\theta} $ is Normal with mean $ \mu $ and variance $ \sigma^2 $.
   - MLE for $ \mu $: $ \hat{\mu}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^{n} x_i $ (Sample Mean).
   - MLE for $ \sigma^2 $: $ \hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{\mu}_{\text{MLE}})^2 $ (Sample Variance).

**Gaussian Mixture Model (Preview):**
- A more complex model involving multiple normal distributions.
- MLE estimation involves finding parameters for each normal distribution in the mixture.
- Applications in clustering problems.

**Conclusion:**
- MLE provides a systematic way to estimate parameters.
- Easily applicable to various probability distributions.
- Useful foundation for more advanced statistical modeling. 

References
- [MLE MIT OCW](https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2014/4a8de32565ebdefbb7963b4ebda904b2_MIT18_05S14_Reading10b.pdf)
- [MLE Proof based](https://www.statlect.com/fundamentals-of-statistics/maximum-likelihood)