# Constrained Optimization P2

1. **Context of Constrained Optimization:**
   - The discussion begins with the consideration of optimization problems with constraints.
   - In unconstrained optimization, gradient descent is a viable method.

2. **Necessary Condition for Optimality:**
   - The lecture introduces the necessary condition for optimality in the presence of constraints.
   - For an optimal solution $x^{\ast}$, the gradients $\nabla f(x^{\ast})$ and $\nabla g(x^{\ast})$ should be related.
   - The relationship is stated as $\nabla f(x^{\ast}) = -\lambda \nabla g(x^{\ast})$, where $\lambda$ is a Lagrange multiplier.

3. **Case of Inequality Constraint:**
   - In the case of an inequality constraint ($g(x) \leq 0$), $\lambda$ is constrained to be positive.

4. **No Descent Direction Should be Feasible:**
   - The lecture emphasizes the significance of the condition. If there exists a feasible direction in which the objective function decreases ($\nabla f(x^{\ast})$ points), the solution $x^{\ast}$ cannot be optimal.

5. **Equality Constraint Case:**
   - The discussion extends to the case of an equality constraint ($g(x) = 0$).
   - The conditions for optimality involve both anti-parallel ($\nabla f(x^{\ast}) = -\lambda \nabla g(x^{\ast})$) and parallel ($\nabla f(x^{\ast}) = \lambda \nabla g(x^{\ast})$) situations.
   - In the equality case, $\lambda$ is allowed to be any scalar.

6. **Lagrange Multiplier:**
   - The introduced scalar $\lambda$ is termed the Lagrange multiplier.

7. **Summary and Application:**
   - The lecture concludes by stating that this necessary condition helps identify potential optimal solutions.
   - The next steps in the course might involve using Lagrange multipliers to solve specific optimization problems, particularly those with equality constraints.  
   
# Linear Approximate  
Given a function $f:\mathbb{R}^n \rightarrow \mathbb{R}$, the linear approximation of a function $f$ at the point $(x + \epsilon d)$ is $f(x) + \epsilon d^T\nabla f(x)$