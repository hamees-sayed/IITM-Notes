# Application of Optimization in Machine Learning  

1. **Convexity:**
   - Convexity is a crucial concept in optimization, characterized by a function where the line segment connecting any two points in its domain lies above the function's graph.
   - Convex functions exhibit favorable properties, making them suitable for optimization problems.

2. **Linear Regression as a Convex Problem:**
   - The linear regression problem is presented as a convex optimization problem.
   - Objective: Minimize the sum of squared differences between predicted and actual values.
   - Solution: Identify weights (w) that minimize the convex objective function.

   $\text{Minimize } f(w) = \frac{1}{2} \|Xw - y\|^2$  
   
   - So, the optimal solution for the linear regression problem, where we minimize the sum of squared errors, is given by:  

    $w^* = (X^T X)^{-1} X^T y$

    - This formula provides the weights that minimize the sum of squared errors, making it the optimal solution for linear regression. It's worth noting that the pseudo-inverse is used to handle cases where $X^T X$ may not be invertible.

3. **Iterative Methods and Gradient Descent:**
   - Iterative methods, such as gradient descent, are applicable to convex optimization problems.
   - Gradient descent updates weights iteratively, moving in the direction of the negative gradient.
   - Convergence of gradient descent is guaranteed for convex functions.

   $w_{t+1} = w_t - \eta_t \nabla f(w_t)$   
   $\nabla f(w) = (X^TX)w - X^Ty$

4. **Challenges with Inverse Computation:**
   - Analytical solutions may involve inverse computation, which can be computationally expensive for high-dimensional data.
   - In linear regression, finding the optimal weights ($w$) can require the computation of the inverse of the matrix $X^TX$.
   - If $X^TX$ is a $d \times d$ matrix, the inverse operation is $O(d^3)$.
   - High $d$ values, common in machine learning problems like image recognition, lead to impractical computation times.

5. **Stochastic Gradient Descent (SGD):**
   - To address computational challenges, stochastic gradient descent is introduced.
   - SGD employs random sampling of a small uniformly distributed subset of data points at each iteration, approximating the gradient converging to global minima.
   - The algorithm updates weights iteratively based on this sampled gradient.

   $\nabla f(w_t) \approx \nabla f_i(w_t)$    
   $w_{t+1} = w_t - \eta_t \nabla f_i(w_t)$

6. **Benefits of SGD:**
   - SGD offers a practical solution for large-scale machine learning problems, avoiding inverse computations and using only a subset of data points for gradient updates.
   - The algorithm may not always converge to the optimal solution but oscillate close to it where the difference is negligible and efficient for most Machine Learning Problems.