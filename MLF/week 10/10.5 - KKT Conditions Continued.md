# KKT Conditions Continued

**Introduction:**
- Emphasis on a fundamental algorithm, Support Vector Machine (SVM), which is expressed as an optimization problem.

**Support Vector Machine (SVM):**
- SVM is a classical algorithm in machine learning, often encountered in ML theory courses.
- Formulation as an optimization problem: $\text{min}_w \frac{1}{2} ||w||^2$ subject to $w^Tx_iy_i \geq 1 \space \forall \space i$.
- The algorithm aims to find a vector $w$ that minimizes the objective while satisfying linear constraints.

**Connection to Optimization:**
- SVM's problem is a constrained optimization problem with a convex quadratic objective and linear constraints. Where $\text{min}_w \frac{1}{2} ||w||^2$ is a quadratic/convex objective function and $w^Tx_iy_i$ is the linear constraint.
- Linear constraints allow the application of strong duality, swapping the min and max, leading to the dual problem.

**Benefits of Dual Problem:**
- Solving the dual problem provides insights into the SVM algorithm.
- Transition from linear to non-linear models smoothly.
- Dual problems are often easier to solve, leading to the development of kernel methods.

**Relevance in Machine Learning:**
- Highlights the importance of understanding the relation between primal and dual problems in machine learning.
- Many ML problems involve convex objectives, enabling the use of duality and efficient solution methods.
- Examples include linear regression and SVM, showcasing applications in machine learning.