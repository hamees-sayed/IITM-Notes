# Algorithm for Gradient Descent  
 $\text{min} \space f(x), x \ in \mathbb{R}$  

**Algorithm:**  
Initialize at any arbitrary $x_0 \in \mathbb{R}$   
for $t = 1, 2, \dots n$  
$\space \space \space \space x_{t+1} = x_t - \eta_t f'(x_t)$  
end  

Where, $\eta$ is the step size and its value is $\eta_t = \frac{1}{1+t}$  

## Properties of Gradient Descent  
1. If $\eta_t = \frac{1}{1+t}$, t he algorithm converges.  

2. Gradient Descent converges to local minimum, $f(x^*) \leq  f(x) \space \forall \space x \dots \space (x^* \space \text{is value of x at global minimum})$  

3. For $x$ to be local minimum, $\exists \space \epsilon > 0$ such that $f(x^*) \leq f(x) \space \forall \space x \in [x^*-\epsilon, \space x^*+\epsilon]$  

4. For some functions, local minimum = global minimum. These functions are known as convex functions.  
If $f''(x) \geq 0 \space \forall \space x \in [x-\epsilon, \space x+\epsilon]$, then the function is convex.  
